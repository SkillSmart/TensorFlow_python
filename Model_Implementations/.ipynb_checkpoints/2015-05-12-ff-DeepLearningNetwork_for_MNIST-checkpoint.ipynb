{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a deep learning Architecture\n",
    "\n",
    "*Author*: Frank Fichtenmueller <br>\n",
    "*Goal*: Sample Implementation to learn about the Syntax of Tensorflow<br>\n",
    "*Date*: 12/05/2017\n",
    "\n",
    "<hr>\n",
    "Using multiple layers of networks, the goal is to enable the network to learn 2-D Spacial Representation Features to improve the accuracy of the prediction. \n",
    "\n",
    "Building on top of [2015-05-12-ff-NeuralNetwork](http://localhost:8891/notebooks/Model_Implementations/2017-05-12-ff-NeuralNetwork.ipynb) we will now implement the picture layout by using a 'convolutional neural network' to compress and learn spacial features to help increase accuracy in distinguishing the harder to decipher parts of the data.\n",
    "\n",
    "Architecture: <br>\n",
    "- A convolutional layer learns on spacial subsets of the image representation, and over time will generalize to a 2-tensor for a specific digit shape. \n",
    "- A Pooling layer is then trained to compress the digit generalization into a smaller subset of patterns, to force a bottleneck to keep the model from overfitting the specifics and increase generalization\n",
    "- [convolution , pooling] is repeated twice. The second combination will be learning conceptual patterns of the arrangement of the first combinations generalized patterns. Therefore learning more abstract patterns.\n",
    "- The output is then fed into a fully connected layer to train the weights and biases to combine the individual features towards classification results.\n",
    "- 10 individual Neurons are set up with a Softmax Function for multi-class classification to maximize the logistic output seperation between high and low valued predictions. \n",
    "- The last layer implements the 'loss function' to measure accuracy, and initiates the backpropagation function to adjust the weights and bias terms on the fully connected layer, which in turn sends adjusted derivatives down to the next layer. This continues trough all layers.\n",
    "\n",
    "Reduce Overfitting: <br>\n",
    "- Our Model has enough degrees of freedom to perfectly learn all relevant features within our training data. Likelihood to overfitting sample specifics is therefore high. \n",
    "- We use 'dropout' on the Fully connected layer to force the classification algorithm to learn distributed submodels on the same data and not rely too much on the presence of specific features (Nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-41389fad42b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Get Data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define path to TensorBoard log files\n",
    "logPath = \"./tb_logs/\"\n",
    "\n",
    "# Define a function that collects statistics for TensorBoard\n",
    "def variable_summaries(var):\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('mean', mean)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var-mean)))\n",
    "        tf.summary.scalar('stddev', stddev)\n",
    "        tf.summary.scalar('max', tf.reduce_max(var))\n",
    "        tf.summary.scalar('min', tf.reduce_min(var))\n",
    "        tf.summary.histogram('histogram', var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('MNIST_Input'):\n",
    "    # Define the placeholders for MNIST input data\n",
    "    x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "    y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "with tf.name_scope('Input_Reshape'):\n",
    "    # Reshaping the flattened vector in a 2-tensor\n",
    "    x_image = tf.reshape(x, [-1, 28,28,1], name='x_image')\n",
    "    tf.summary.image('input_img', x_image, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For our activation function we use 'ReLu', therefor we need to initialize\n",
    "# with small random values, so that Relu does not cancel them out right away\n",
    "\n",
    "def weight_variable(shape, name=None):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n",
    "def bias_variable(shape, name=None):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial, name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We isolate the creation of the convolution and pooling layers, so that we can easily set parameters on the whole network in a single place. \n",
    "\n",
    "- Convolution Layers set a stride, and the padding\n",
    "- Max Pooling sets the Kernel Size which determines the size of the array we are pooling together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create functions to set up convolution and pooling layers for us\n",
    "def conv2d(x, W, name=None):\n",
    "    return tf.nn.conv2d(x, W, strides=[1,1,1,1], padding='SAME', name=name)\n",
    "\n",
    "def max_pool_2x2(x, name=None):\n",
    "    return tf.nn.max_pool(x, ksize=[1,2,2,1],\n",
    "                         strides=[1,2,2,1], padding='SAME', name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Layers of the Neural Network\n",
    "\n",
    "We initialize the layers and implement the architectural definitions by setting parameters to the model layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Convolutional Layer\n",
    "\n",
    "Given our decission to convolute on a patch of 5x5 we will end up with 32 individiual features per image, that will be attributed with a specific weight, and an individual bias term. \n",
    "\n",
    "- Therefore we create a 4-tensor Weigh Matrix 'W_conv1': [5,5,1,32]\n",
    "    - 5x5 input size\n",
    "    - 1 channel (for greyscale)\n",
    "    - 32 Features in size\n",
    "- A 1-tensor bias variable 'b_conv1': [32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('Conv1'):\n",
    "    with tf.name_scope('weights'):\n",
    "        W_conv1 = weight_variable([5,5,1,32], name='weight')\n",
    "        variable_summaries(W_conv1)\n",
    "    with tf.name_scope('bias'):\n",
    "        b_conv1 = bias_variable([32], name='bias')\n",
    "        variable_summaries(b_conv)\n",
    "\n",
    "    # Do convolution on images, add bias and push through RELU activation\n",
    "    conv1_wx_b = conv2d(x_image, W_conv1, name='conv2d') + b_conv1\n",
    "    tf.summary.histogram('conv1_wx_b', conv1_wx_b)\n",
    "    h_conv1 = tf.nn.relu(conv1_wx_b, name='relu')\n",
    "    tf.summary.histogram('h_conv1', h_conv1)\n",
    "    # Take results and run them trough max_pool\n",
    "    h_pool1 = max_pool_2x2(h_conv1, name='pool')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Convolutional Layer\n",
    "\n",
    "This layer processes the output of layer 1 in a 5x5 patch. Returning 64 Weights and Bias Terms.\n",
    "\n",
    "- Therefore we create a 4-tensor Weigh Matrix 'W_conv1': [5,5,1,32]\n",
    "    - 5x5 input size\n",
    "    - 32 channel (Features from Layer one)\n",
    "    - 64 Features Output\n",
    "- A 1-tensor bias variable 'b_conv1': [32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-891816c07b40>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Conv2'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[1;31m# Process the 32 features from  Conv1 in a 5x5 patch. Return 64 Weights and bias\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'weights'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mW_conv2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweight_variable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'weight'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mvariable_summaries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mW_conv2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "with tf.name_scope('Conv2'):\n",
    "    # Process the 32 features from  Conv1 in a 5x5 patch. Return 64 Weights and bias\n",
    "    with tf.name_scope('weights'):\n",
    "        W_conv2 = weight_variable([5,5,32,64], name='weight')\n",
    "        variable_summaries(W_conv2)\n",
    "    with tf.name_scope('bias'):\n",
    "        b_conv2 = bias_variable([64], name='bias')\n",
    "        variable_summaries(b_conv2)\n",
    "    \n",
    "    # Do convolution on the output of layer 1. Pool results\n",
    "    conv2_wx_b = conv2d(h_pool1, W_conv2, name='conv2d') + b_conv2\n",
    "    tf.summary.histogram('conv2_wx_b', conv2_wx_b)\n",
    "    h_conv2 = tf.nn.relu(conv2_wx_b, name='relu')\n",
    "    tf.summary.histogram('h_conv2', h_conv2)\n",
    "    h_pool2 = max_pool_2x2(h_conv2, name='pool')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Implement a fully connected Layer\n",
    "\n",
    "This Layer receives a 7x7 Representation of the images, and outputs its weights to 10 propability function to classify the labels 0-9.\n",
    "\n",
    "- Input is 7x7 images with 64 Features\n",
    "- Connection of the whole system is 1024 Neurons all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('FC'):\n",
    "    # Implementing the Fully Connected Layer\n",
    "    W_fc1 = weight_variable([7*7*64, 1024], name='weight')\n",
    "    b_fc1 = bias_variable([1024], name='bias')\n",
    "\n",
    "    # Connect output of pooling layer 2 as input to full connected layer\n",
    "    h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1, name='relu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As this very powerfull model can easily overfitt the comparably small dataset we use for training it, we need to implement a 'Dropout' on the fully connected layer, before passing the results to the Classification Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Implementing the 'Readout Layer'\n",
    "\n",
    "This Layer takes the values and computes probability Statements about the Class prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('Readout'):\n",
    "    # Implementing the Layer\n",
    "    W_fc2 = weight_variable([1024, 10], name='weight')\n",
    "    b_fc2 = bias_variable([10], name='bias')\n",
    "\n",
    "    # Defining the model\n",
    "    y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing the 'loss function' to calculate back propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('cross_entropy'):\n",
    "    # Loss measurement\n",
    "    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits=y_conv, labels=y_))\n",
    "\n",
    "with tf.name_scope('loss_optimizer'):\n",
    "    # loss optimization\n",
    "    train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the accuracy Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('Accuracy'):\n",
    "    # What is correct?\n",
    "    correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_,1))\n",
    "    # How accurate\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "# Add functions to collect summary on the accuracy changes\n",
    "tf.summary.scalar('cross_entropy_scl', cross_entropy)\n",
    "tf.summary.scalar('training_accuracy', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TB - merge summaries\n",
    "summarize_all = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize all of the variables\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# TB - Write the default graph out to view its structure\n",
    "tbWriter = tf.summary.FileWriter(logPath, sess.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, elapsed time  0.37 seconds, training accuracy  4.000%\n",
      "step 100, elapsed time  17.90 seconds, training accuracy  90.000%\n",
      "step 200, elapsed time  35.29 seconds, training accuracy  94.000%\n",
      "step 300, elapsed time  52.54 seconds, training accuracy  90.000%\n",
      "step 400, elapsed time  69.83 seconds, training accuracy  90.000%\n",
      "step 500, elapsed time  87.16 seconds, training accuracy  92.000%\n",
      "step 600, elapsed time  104.43 seconds, training accuracy  94.000%\n",
      "step 700, elapsed time  121.65 seconds, training accuracy  96.000%\n",
      "step 800, elapsed time  138.92 seconds, training accuracy  96.000%\n",
      "step 900, elapsed time  156.18 seconds, training accuracy  98.000%\n"
     ]
    }
   ],
   "source": [
    "# Set variables to controll the training iterations\n",
    "import time\n",
    "num_steps = 1000\n",
    "display_every = 100\n",
    "\n",
    "# Training Loop\n",
    "start_time = time.time()\n",
    "end_time = time.time()\n",
    "\n",
    "for i in range(num_steps):\n",
    "    batch = mnist.train.next_batch(50)\n",
    "    _, summary = sess.run([train_step, summarize_all], feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n",
    "    \n",
    "    # Periodic status display\n",
    "    if i%display_every == 0:\n",
    "        train_accuracy = accuracy.eval(feed_dict= {\n",
    "            x:batch[0], y_: batch[1], keep_prob: 1.0})\n",
    "        end_time = time.time()\n",
    "        print(\"step {0}, elapsed time {1: .2f} seconds, training accuracy {2: .3f}%\".\n",
    "              format(i, end_time-start_time, train_accuracy* 100))\n",
    "        #Write summary to log\n",
    "        tbWriter.add_summary(summary, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Display summary\n",
    "end_time = time.time()\n",
    "print('Total training time for {0} batches: {1:.2f} seconds'.format(i+1, end_time-start_time))\n",
    "\n",
    "# Accuracy on the test set\n",
    "print(\"Test accuracy {0:.3f}%\".format(accuracy.eval(feed_dict={\n",
    "    x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0\n",
    "})*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!tensorboard --log tb_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a deep learning Architecture\n",
    "\n",
    "*Author*: Frank Fichtenmueller <br>\n",
    "*Goal*: Sample Implementation to learn about the Syntax of Tensorflow<br>\n",
    "*Date*: 12/05/2017\n",
    "\n",
    "<hr>\n",
    "Using multiple layers of networks, the goal is to enable the network to learn 2-D Spacial Representation Features to improve the accuracy of the prediction. \n",
    "\n",
    "Building on top of [2015-05-12-ff-NeuralNetwork](http://localhost:8891/notebooks/Model_Implementations/2017-05-12-ff-NeuralNetwork.ipynb) we will now implement the picture layout by using a 'convolutional neural network' to compress and learn spacial features to help increase accuracy in distinguishing the harder to decipher parts of the data.\n",
    "\n",
    "Architecture: <br>\n",
    "- A convolutional layer learns on spacial subsets of the image representation, and over time will generalize to a 2-tensor for a specific digit shape. \n",
    "- A Pooling layer is then trained to compress the digit generalization into a smaller subset of patterns, to force a bottleneck to keep the model from overfitting the specifics and increase generalization\n",
    "- [convolution , pooling] is repeated twice. The second combination will be learning conceptual patterns of the arrangement of the first combinations generalized patterns. Therefore learning more abstract patterns.\n",
    "- The output is then fed into a fully connected layer to train the weights and biases to combine the individual features towards classification results.\n",
    "- 10 individual Neurons are set up with a Softmax Function for multi-class classification to maximize the logistic output seperation between high and low valued predictions. \n",
    "- The last layer implements the 'loss function' to measure accuracy, and initiates the backpropagation function to adjust the weights and bias terms on the fully connected layer, which in turn sends adjusted derivatives down to the next layer. This continues trough all layers.\n",
    "\n",
    "Reduce Overfitting: <br>\n",
    "- Our Model has enough degrees of freedom to perfectly learn all relevant features within our training data. Likelihood to overfitting sample specifics is therefore high. \n",
    "- We use 'dropout' on the Fully connected layer to force the classification algorithm to learn distributed submodels on the same data and not rely too much on the presence of specific features (Nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Get Data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the placeholders for MNIST input data\n",
    "x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# Reshaping the flattened vector in a 2-tensor\n",
    "x_image = tf.reshape(x, [-1, 28,28,1], name='x_image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For our activation function we use 'ReLu', therefor we need to initialize\n",
    "# with small random values, so that Relu does not cancel them out right away\n",
    "\n",
    "def weight_variable(shape, name=None):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n",
    "def bias_variable(shape, name=None):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial, name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We isolate the creation of the convolution and pooling layers, so that we can easily set parameters on the whole network in a single place. \n",
    "\n",
    "- Convolution Layers set a stride, and the padding\n",
    "- Max Pooling sets the Kernel Size which determines the size of the array we are pooling together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create functions to set up convolution and pooling layers for us\n",
    "def conv2d(x, W, name=None):\n",
    "    return tf.nn.conv2d(x, W, strides=[1,1,1,1], padding='SAME', name=name)\n",
    "\n",
    "def max_pool_2x2(x, name=None):\n",
    "    return tf.nn.max_pool(x, ksize=[1,2,2,1],\n",
    "                         strides=[1,2,2,1], padding='SAME', name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Layers of the Neural Network\n",
    "\n",
    "We initialize the layers and implement the architectural definitions by setting parameters to the model layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Convolutional Layer\n",
    "\n",
    "Given our decission to convolute on a patch of 5x5 we will end up with 32 individiual features per image, that will be attributed with a specific weight, and an individual bias term. \n",
    "\n",
    "- Therefore we create a 4-tensor Weigh Matrix 'W_conv1': [5,5,1,32]\n",
    "    - 5x5 input size\n",
    "    - 1 channel (for greyscale)\n",
    "    - 32 Features in size\n",
    "- A 1-tensor bias variable 'b_conv1': [32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('Conv1'):\n",
    "    W_conv1 = weight_variable([5,5,1,32], name='weight')\n",
    "    b_conv1 = bias_variable([32], name='bias')\n",
    "\n",
    "    # Do convolution on images, add bias and push through RELU activation\n",
    "    h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1, name='conv2d') + b_conv1, name='relu')\n",
    "    # Take results and run them trough max_pool\n",
    "    h_pool1 = max_pool_2x2(h_conv1, name='pool')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Convolutional Layer\n",
    "\n",
    "This layer processes the output of layer 1 in a 5x5 patch. Returning 64 Weights and Bias Terms.\n",
    "\n",
    "- Therefore we create a 4-tensor Weigh Matrix 'W_conv1': [5,5,1,32]\n",
    "    - 5x5 input size\n",
    "    - 32 channel (Features from Layer one)\n",
    "    - 64 Features Output\n",
    "- A 1-tensor bias variable 'b_conv1': [32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('Conv2'):\n",
    "    # Process the 32 features from  Conv1 in a 5x5 patch. Return 64 Weights and bias\n",
    "    W_conv2 = weight_variable([5,5,32,64], name='weight')\n",
    "    b_conv2 = bias_variable([64], name='bias')\n",
    "    # Do convolution on the output of layer 1. Pool results\n",
    "    h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2, name='conv2d') + b_conv2, name='relu')\n",
    "    h_pool2 = max_pool_2x2(h_conv2, name='pool')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Implement a fully connected Layer\n",
    "\n",
    "This Layer receives a 7x7 Representation of the images, and outputs its weights to 10 propability function to classify the labels 0-9.\n",
    "\n",
    "- Input is 7x7 images with 64 Features\n",
    "- Connection of the whole system is 1024 Neurons all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('FC'):\n",
    "    # Implementing the Fully Connected Layer\n",
    "    W_fc1 = weight_variable([7*7*64, 1024], name='weight')\n",
    "    b_fc1 = bias_variable([1024], name='bias')\n",
    "\n",
    "    # Connect output of pooling layer 2 as input to full connected layer\n",
    "    h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1, name='relu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As this very powerfull model can easily overfitt the comparably small dataset we use for training it, we need to implement a 'Dropout' on the fully connected layer, before passing the results to the Classification Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Implementing the 'Readout Layer'\n",
    "\n",
    "This Layer takes the values and computes probability Statements about the Class prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('Readout'):\n",
    "    # Implementing the Layer\n",
    "    W_fc2 = weight_variable([1024, 10], name='weight')\n",
    "    b_fc2 = bias_variable([10], name='bias')\n",
    "\n",
    "    # Defining the model\n",
    "    y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing the 'loss function' to calculate back propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss measurement\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=y_conv, labels=y_))\n",
    "\n",
    "# loss optimization\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the accuracy Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# What is correct?\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_,1))\n",
    "# How accurate\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize all of the variables\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, elapsed time  0.24 seconds, training accuracy  22.000%\n",
      "step 100, elapsed time  17.84 seconds, training accuracy  84.000%\n",
      "step 200, elapsed time  35.40 seconds, training accuracy  92.000%\n",
      "step 300, elapsed time  52.90 seconds, training accuracy  86.000%\n",
      "step 400, elapsed time  70.40 seconds, training accuracy  94.000%\n",
      "step 500, elapsed time  87.91 seconds, training accuracy  86.000%\n",
      "step 600, elapsed time  105.41 seconds, training accuracy  98.000%\n",
      "step 700, elapsed time  122.80 seconds, training accuracy  94.000%\n",
      "step 800, elapsed time  140.26 seconds, training accuracy  94.000%\n",
      "step 900, elapsed time  157.66 seconds, training accuracy  92.000%\n",
      "step 1000, elapsed time  175.04 seconds, training accuracy  98.000%\n",
      "step 1100, elapsed time  192.58 seconds, training accuracy  96.000%\n",
      "step 1200, elapsed time  210.09 seconds, training accuracy  98.000%\n",
      "step 1300, elapsed time  227.57 seconds, training accuracy  98.000%\n",
      "step 1400, elapsed time  245.09 seconds, training accuracy  96.000%\n",
      "step 1500, elapsed time  262.45 seconds, training accuracy  96.000%\n",
      "step 1600, elapsed time  279.78 seconds, training accuracy  98.000%\n",
      "step 1700, elapsed time  297.11 seconds, training accuracy  100.000%\n",
      "step 1800, elapsed time  314.45 seconds, training accuracy  100.000%\n",
      "step 1900, elapsed time  331.93 seconds, training accuracy  100.000%\n",
      "step 2000, elapsed time  349.29 seconds, training accuracy  100.000%\n",
      "step 2100, elapsed time  366.63 seconds, training accuracy  98.000%\n",
      "step 2200, elapsed time  384.09 seconds, training accuracy  94.000%\n",
      "step 2300, elapsed time  401.40 seconds, training accuracy  100.000%\n",
      "step 2400, elapsed time  418.75 seconds, training accuracy  100.000%\n",
      "step 2500, elapsed time  436.07 seconds, training accuracy  94.000%\n",
      "step 2600, elapsed time  453.41 seconds, training accuracy  100.000%\n",
      "step 2700, elapsed time  470.73 seconds, training accuracy  98.000%\n",
      "step 2800, elapsed time  488.11 seconds, training accuracy  98.000%\n",
      "step 2900, elapsed time  505.44 seconds, training accuracy  98.000%\n"
     ]
    }
   ],
   "source": [
    "# Set variables to controll the training iterations\n",
    "import time\n",
    "num_steps = 3000\n",
    "display_every = 100\n",
    "\n",
    "# Training Loop\n",
    "start_time = time.time()\n",
    "end_time = time.time()\n",
    "\n",
    "for i in range(num_steps):\n",
    "    batch = mnist.train.next_batch(50)\n",
    "    train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n",
    "    \n",
    "    # Periodic status display\n",
    "    if i%display_every == 0:\n",
    "        train_accuracy = accuracy.eval(feed_dict= {\n",
    "            x:batch[0], y_: batch[1], keep_prob: 1.0})\n",
    "        end_time = time.time()\n",
    "        print(\"step {0}, elapsed time {1: .2f} seconds, training accuracy {2: .3f}%\".\n",
    "              format(i, end_time-start_time, train_accuracy* 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training time for 3000 batches: 525.46 seconds\n",
      "Test accuracy 98.100%\n"
     ]
    }
   ],
   "source": [
    "# Display summary\n",
    "end_time = time.time()\n",
    "print('Total training time for {0} batches: {1:.2f} seconds'.format(i+1, end_time-start_time))\n",
    "\n",
    "# Accuracy on the test set\n",
    "print(\"Test accuracy {0:.3f}%\".format(accuracy.eval(feed_dict={\n",
    "    x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0\n",
    "})*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
